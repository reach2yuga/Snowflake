Question : 1
Execution Time (8m 20.81s) => Total time: 8 minutes 20.81 seconds — quite long for most queries.
Processing: 77% => 77% of the time is spent on processing, indicating computational bottlenecks.
Local Disk IO: 2%
Synchronization: 21%
Initialization: 1%
(100%)

Statistics

Scan progress: 100.00%
Bytes scanned: 5.61 GB
Percentage scanned from cache: 100.00% =>100% scanned from cache — good, but doesn’t help if query logic is inefficient.
Bytes written: 0.81 MB

Pruning
Partitions scanned: 1,328 => No pruning occurred, which is a red flag. This means every partition was scanned, increasing the workload.
Partitions total: 1,328

Spilling
Bytes spilled to local storage: 37.59 GB =>Indicates memory pressure; the data couldn't be processed in-memory, likely causing slowdowns.

Answer:  Increase the size of the virtual warehouse.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Question 2:
How can the following relational data be transformed into semi-structured data using the
LEAST amount of operational overhead?

| Row | PROVINCE | CREATED\_DATE |
| --- | -------- | ------------- |
| 2   | Alberta  | 2020-01-19    |
| 1   | Manitoba | 2020-01-18    |

Answer:  Use the OBJECT_CONSTRUCT function to return a Snowflake object.
SELECT OBJECT_CONSTRUCT('province', province, 'created_date', created_date) FROM provinces; // { "province": "Alberta", "created_date": "2020-01-19" }

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

A Data Engineer executes a complex query and wants to make use of Snowflake’s
query results caching capabilities to reuse the results.
Which conditions must be met? (Choose three.)
A. The results must be reused within 72 hours.
B. The query must be executed using the same virtual warehouse.
C. The USED_CACHED_RESULT parameter must be included in the query.
D. The table structure contributing to the query result cannot have changed.
E. The new query must have the same syntax as the previously executed query.
F. The micro-partitions cannot have changed due to changes to other data in the
table.
Answer D, E, F

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question : 4
A Data Engineer needs to load JSON output from some software into Snowflake using
Snowpipe.
Which recommendations apply to this scenario? (Choose three.)
A. Load large files (1 GB or larger).
B. Ensure that data files are 100-250 MB (or larger) in size, compressed.
C. Load a single huge array containing multiple records into a single table row.
D. Verify each value of each unique element stores a single native data type
(string or number).
E. Extract semi-structured data elements containing null values into relational
columns before loading.
F. Create data files that are less than 100 MB and stage them in cloud storage at
a sequence greater than once each minute.
Answer : B, D , E

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Qusetion 5
Given the table SALES which has a clustering key of column CLOSED_DATE, which
table function will return the average clustering depth for the
SALES_REPRESENTATIVE column for the North American region?

A. select system$clustering_information('Sales', 'sales_representative', 'region =''North America''');
B. select system$clustering_depth('Sales', 'sales_representative', 'region =''North America''');
C. select system$clustering_depth('Sales', 'sales_representative') where region ='North America';
D. select system$clustering_information('Sales', 'sales_representative') whereregion = 'North America’;

Answer : B

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Qustion 6:
A large table with 200 columns contains two years of historical data. When queried, the
table is filtered on a single day. Below is the Query Profile:

Using a size 2XL virtual warehouse, this query took over an hour to complete.
What will improve the query performance the MOST?
A. Increase the size of the virtual warehouse.
B. Increase the number of clusters in the virtual warehouse.
C. Implement the search optimization service on the table.
D. Add a date column as a cluster key on the table.
Answer : D

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Question 7:
A Data Engineer is working on a Snowflake deployment in AWS eu-west-1 (Ireland).
The Engineer is planning to load data from staged files into target tables using the
COPY INTO command.
Which sources are valid? (Choose three.)
A. Internal stage on GCP us-central1 (Iowa)
B. Internal stage on AWS eu-central-1 (Frankfurt)
C. External stage on GCP us-central1 (Iowa)
D. External stage in an Amazon S3 bucket on AWS eu-west-1 (Ireland)
E. External stage in an Amazon S3 bucket on AWS eu-central-1 (Frankfurt)
F. SSD attached to an Amazon EC2 instance on AWS eu-west-1 (Ireland)
Answer : C, D , E

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question 8:
A Data Engineer wants to create a new development database (DEV) as a clone of the
permanent production database (PROD). There is a requirement to disable Fail-safe for
all tables.
Which command will meet these requirements?
A. CREATE DATABASE DEV -
CLONE PROD -
FAIL_SAFE = FALSE;
B. CREATE DATABASE DEV -
CLONE PROD;
C. CREATE TRANSIENT DATABASE DEV -
CLONE PROD;
D. CREATE DATABASE DEV -
CLONE PROD -
DATA_RETENTION_TIME_IN DAYS = 0;
Answer : C
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question 10:
Which methods can be used to create a DataFrame object in Snowpark? (Choose
three.)
A. session.jdbc_connection()
B. session.read.json()
C. session.table()
D. DataFrame.write()
E. session.builder()
F. session.sql()
Answer : B , C , F

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question : 11
A new CUSTOMER table is created by a data pipeline in a Snowflake schema where
MANAGED ACCESS is enabled.
Which roles can grant access to the CUSTOMER table? (Choose three.)
A. The role that owns the schema
B. The role that owns the database
C. The role that owns the CUSTOMER table
D. The SYSADMIN role
E. The SECURITYADMIN role
F. The USERADMIN role with the MANAGE GRANTS privilege
Answer : A, E , F

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question : 12
What is the purpose of the BUILD_STAGE_FILE_URL function in Snowflake?
A. It generates an encrypted URL for accessing a file in a stage.
B. It generates a staged URL for accessing a file in a stage.
C. It generates a permanent URL for accessing files in a stage.
D. It generates a temporary URL for accessing a file in a stage.
Answer : C

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Question : 14
A company has an extensive script in Scala that transforms data by leveraging
DataFrames. A Data Engineer needs to move these transformations to Snowpark.
What characteristics of data transformations in Snowpark should be considered to meet
this requirement? (Choose two.)
A. It is possible to join multiple tables using DataFrames.
B. Snowpark operations are executed lazily on the server.
C. User-Defined Functions (UDFs) are not pushed down to Snowflake.
D. Snowpark requires a separate cluster outside of Snowflake for computations.
E. Columns in different DataFrames with the same name should be referred to
with squared brackets.
Ans: A,B
