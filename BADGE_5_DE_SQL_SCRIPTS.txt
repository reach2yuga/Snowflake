-- PROJECT RELATED TO SNOWFLAKE BADGE 5 TRAINING.
-- The main work of a data Engineer is to build ETL pipelines. ETL stands for Extract, Transform and Load. The work of a Data Engineer is to take RAW data and refine that data until it matches what the customer* is looking for.
-- WE WILL MAKE USE OF DIFFERENT SCHEMAS CALLED - RAW, ENHANCED, CURATED

-- CREATING A DATABASE
create database AGS_GAMES;

-- DROPPING PUBLIC SCHEMA 
drop schema AGS_GAMES.PUBLIC;

--CREATING NEW SCHEMA CALLED 'RAW'
create schema AGS_GAMES.RAW;

--CREATING A 'GAME_LOGS' TABLE WITH ONE COLUMN TO LOAD DATA INTO IT FROM EXTERNAL STAGE
create or replace TABLE AGS_GAMES.RAW.GAME_LOGS (
	RAW_LOG VARIANT
);

--CREATING AN EXTERNAL STAGE (S3)
CREATE STAGE UNI_KISHORE 
	URL = 's3://uni-kishore' 
	DIRECTORY = ( ENABLE = true );

--LISTING STAGE CONTENTS
list @uni_kishore/kickoff;

--CREATING A FILE FORMAT
create file format FF_JSON_LOGS
    type = 'JSON'
     strip_outer_array = true;

-- QUERYING THE FILES WHILE THEY ARE PRESENT AT STAGE LEVEL ONLY USING THE FILE FORMAT WE CREATED.
 select $1
 from @uni_kishore/kickoff
 (file_format => AGS_GAMES.RAW.FF_JSON_LOGS);

-- LOADING DATA TO THE TABLE WE CREATED USING COPY CMND
copy into AGS_GAMES.RAW.GAME_LOGS
from @uni_kishore/kickoff
file_format = (format_name=AGS_GAMES.RAW.FF_JSON_LOGS);

-- RUNNING SELECT STATEMENTS TO QUERY THE RAW_DATA AND STORING IT IN SEPERATE COLUMNS (GAME_LOGS) 
select 
     raw_log:agent::text as agent,
     raw_log:user_event::text as user_event,
     raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
     raw_log:user_login::text as user_login,
     *
 from AGS_GAMES.RAW.GAME_LOGS;

-- STORING THIS SELECTED QUERY RESULT AS A VIEW. TO USE IT FURTHER
create OR REPLACE view AGS_GAMES.RAW.LOGS as
select 
raw_log:agent::text as agent,
raw_log:user_event::text as user_event,
raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
raw_log:user_login::text as user_login,
*
from AGS_GAMES.RAW.GAME_LOGS;

-- CHECKING WHETHER VIEW IS WORKING CORRCTLY OR NOT.
Select * from AGS_GAMES.RAW.LOGS;

--VALIDATING TIME_STAMP FUNCTION
SELECT current_timestamp();

--what time zone is your account(and/or session) currently set to? Is it -0700?
select current_timestamp();

--worksheets are sometimes called sessions -- we'll be changing the worksheet time zone
alter session set timezone = 'UTC';
select current_timestamp();

--how did the time differ after changing the time zone for the worksheet?
alter session set timezone = 'Africa/Nairobi';
select current_timestamp();

alter session set timezone = 'Pacific/Funafuti';
select current_timestamp();

alter session set timezone = 'Asia/Shanghai';
select current_timestamp();

--show the account parameter called timezone
show parameters like 'timezone';

-- QUERYING NEWLY PUBLISHED FILE FROM STAGE 
select $1
from @uni_kishore/updated_feed --THIS FILE DOESN'T CONTAIN AGENT FIELD
(file_format => AGS_GAMES.RAW.FF_JSON_LOGS);

--LOADING THIS DATA TO GAME_LOGS TABLE.
copy into AGS_GAMES.RAW.GAME_LOGS
from @uni_kishore/updated_feed
file_format = (format_name=AGS_GAMES.RAW.FF_JSON_LOGS);

-- RUNNING SELECT STATEMENTS TO QUERY THE RAW_DATA AND STORING IT IN SEPERATE COLUMNS (GAME_LOGS) 
select 
    raw_log:agent::text as agent,
    raw_log:user_event::text as user_event,
    raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
    raw_log:user_login::text as user_login,
    raw_log:ip_address::text as ip_address,
    *
from AGS_GAMES.RAW.GAME_LOGS;

-- ANALYZING THE GAME_LOGS TABLE
select * from AGS_GAMES.RAW.LOGS
where agent is null;

--looking for empty AGENT column
select * 
from ags_games.raw.LOGS
where agent is null;

--looking for non-empty IP_ADDRESS column
select 
RAW_LOG:ip_address::text as IP_ADDRESS
,*
from ags_games.raw.LOGS
where RAW_LOG:ip_address::text is not null;

-- UPDATING LOGS VIEW DEFINITION
create or replace view LOGS as
select 
    raw_log:ip_address::text as ip_address,
    raw_log:user_event::text as user_event,
    raw_log:user_login::text as user_login,
    raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
    *
from AGS_GAMES.RAW.GAME_LOGS
where RAW_LOG:ip_address::text is not null;

--QUERY THE LOGS VIEW FOR UPDATED DATA
SELECT * FROM AGS_GAMES.RAW.LOGS;

-- LOOKING FOR A PARTICULAR USER LOGIN
select * from AGS_GAMES.RAW.LOGS
WHERE USER_LOGIN ilike '%kishore%';

-- WE CAN PULL OUT DIFFERENT VALUES FROM AN IP ADDRESS USING SNOWFLAKE'S PARSE_IP() FUNCTION WITH COLON (:)
 select parse_ip('107.217.231.17','inet');
select parse_ip('107.217.231.17','inet'):ipv4;

-- CREATING A NEW SCHEMA
CREATE SCHEMA ENHANCED;

--Look up Kishore and Prajina's Time Zone in the IPInfo share using his headset's IP Address with the PARSE_IP function.
select start_ip, end_ip, start_ip_int, end_ip_int, city, region, country, timezone
from IPINFO_GEOLOC.demo.location
where parse_ip('100.41.16.160', 'inet'):ipv4 --Kishore's Headset's IP Address
BETWEEN start_ip_int AND end_ip_int;

--Join the log and location tables to add time zone to each row using the PARSE_IP function.
select logs.*
       , loc.city
       , loc.region
       , loc.country
       , loc.timezone
from AGS_GAMES.RAW.LOGS logs
join IPINFO_GEOLOC.demo.location loc
where parse_ip(logs.ip_address, 'inet'):ipv4 
BETWEEN start_ip_int AND end_ip_int;

-- IPINFO_GEOLOC IS A SERVICE THAT WE ARE USING FROM SNOWFLAKE MARKETPLACE. 
-- IT OFFERS A VARIETY OF FUNCTIONS AND DATA TO WORK WITH
--  Use the IPInfo Functions for a More Efficient Lookup()


--Use two functions supplied by IPShare to help with an efficient IP Lookup Process!
SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone 
from AGS_GAMES.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;

-- ADDING A NEW COLUMN CALLED GAME_EVENT_TZ BY USING CCONVERT_TIMEZONE() TO CONVERT THE TIMEZONE AS PER OUR NEEDS.

SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone 
, convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
from AGS_GAMES.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;

-- ADDING A NEW COLUMN CALLED GAME_EVENT_TZ BY USING CCONVERT_TIMEZONE() TO CONVERT THE TIMEZONE AS PER OUR NEEDS.

SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone 
, convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
, DAYNAME(logs.datetime_iso8601) as DOW_NAME
from AGS_GAMES.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;

-- LET US TRY TO KNOW THE GAME IS BEING PLAYED (AT WHAT TIME OF DAY)
-- FOR THIS ,WE'LL CREATE SOME TABLES AS LOOKUP TABLES AND MAKE USE OF ITS VALUES.

--a Look Up table to convert from hour number to "time of day name"
create table ags_games.raw.time_of_day_lu
(  hour number
   ,tod_name varchar(25)
);

--insert statement to add all 24 rows to the table
insert into time_of_day_lu
values
(6,'Early morning'),
(7,'Early morning'),
(8,'Early morning'),
(9,'Mid-morning'),
(10,'Mid-morning'),
(11,'Late morning'),
(12,'Late morning'),
(13,'Early afternoon'),
(14,'Early afternoon'),
(15,'Mid-afternoon'),
(16,'Mid-afternoon'),
(17,'Late afternoon'),
(18,'Late afternoon'),
(19,'Early evening'),
(20,'Early evening'),
(21,'Late evening'),
(22,'Late evening'),
(23,'Late evening'),
(0,'Late at night'),
(1,'Late at night'),
(2,'Late at night'),
(3,'Toward morning'),
(4,'Toward morning'),
(5,'Toward morning');


--Check your table to see if you loaded it properly
select tod_name, listagg(hour,',') 
from time_of_day_lu
group by tod_name;

-- FURTHER UPDATING OUR SELECT STATEMENT BEFORE CREATING A TABLE FROM IT

SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone 
, convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
, DAYNAME(logs.datetime_iso8601) as DOW_NAME
, tod_name
from AGS_GAMES.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
join ags_games.raw.time_of_day_lu lu
on hour(game_event_ltz)= lu.hour;

-- CREATING A TABLE WITH THIS MODIFIED SELECTED DATA

create or replace table ags_games.enhanced.logs_enhanced as
(
    SELECT logs.ip_address
    , logs.user_login as gamer_name
    , logs.user_event as game_event_name
    , logs.datetime_iso8601 as GAME_EVENT_UTC
    , city
    , region
    , country
    , timezone as GAMER_LTZ_NAME
    , convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
    , DAYNAME(game_event_ltz) as DOW_NAME
    , tod_name
    from AGS_GAMES.RAW.LOGS logs
    JOIN IPINFO_GEOLOC.demo.location loc 
    ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
    AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
    BETWEEN start_ip_int AND end_ip_int
    join ags_games.raw.time_of_day_lu lu
    on hour(game_event_ltz)= lu.hour
);

-- Now, we've successfully taken data from a file (extracted it), enhanced it (transformed it) and put it into a database table (loaded it). 

-- Let's try to automate the movement of the data all the way from the external file through to loading of the enhanced table? Generically, this can be referred to as "production-izing" the data load.

-- we will create tasks to do these automation
-- CREATING A SIMPLE TEMPORARY TASK AS OF NOW. WILL MODIFY LATER

create task load_logs_enhanced
    warehouse = 'COMPUTE_WH'
    schedule = '5 minute'
  as
    SELECT 'HELLO';

use role accountadmin;
--You have to run this grant or you won't be able to test your tasks while in SYSADMIN role
--this is true even if SYSADMIN owns the task!!
grant execute task on account to role SYSADMIN;

use role sysadmin; 

--Now you should be able to run the task, even if your role is set to SYSADMIN
execute task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

--the SHOW command might come in handy to look at the task 
show tasks in account;

--you can also look at any task more in depth using DESCRIBE
describe task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

--Run the task a few times to see changes in the RUN HISTORY
execute task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

-- CONFIRMED THAT THE TASK IS RUNNING SUCCESSFULLY.
-- MODIFY THE TASK TO PERFORM ACTUAL WORK

create or replace task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED
	warehouse=COMPUTE_WH
	schedule='5 minute'
	as 
    SELECT 
        logs.ip_address
        , logs.user_login as gamer_name
        , logs.user_event as game_event_name
        , logs.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , tod_name
    from AGS_GAMES.RAW.LOGS logs
    JOIN IPINFO_GEOLOC.demo.location loc 
    ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
    AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
    BETWEEN start_ip_int AND end_ip_int
    join ags_games.raw.time_of_day_lu lu
    on hour(game_event_ltz)= lu.hour;


-- EXECUTE THE TASK MANUALLY TO LOAD MORE ROWS
--make a note of how many rows you have in the table
select count(*)
from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

--Run the task to load more rows
execute task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

--check to see how many rows were added (if any!)
select count(*)
from AGS_GAMES.ENHANCED.LOGS_ENHANCED; 

-- WE ARE GETTING SAME NO OF ROWS AS OUTPUT
--REASON: WE ARE SIMPLY SELECTING THE DATA USING THE TASK WE ARE NOT INSERTING ANYTHING INTO LOAD_LOGS_ENHANCED TABLE WHEN THE TASK IS BEING CALLED.
-- LETS AGAIN MODIFY THE TASK AND RUN

create or replace task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED
	warehouse=COMPUTE_WH
	schedule='5 minute'
	as 
    INSERT INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED 
    SELECT 
        logs.ip_address
        , logs.user_login as gamer_name
        , logs.user_event as game_event_name
        , logs.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , convert_timezone('UTC', timezone,logs.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , tod_name
    from AGS_GAMES.RAW.LOGS logs
    JOIN IPINFO_GEOLOC.demo.location loc 
    ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
    AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
    BETWEEN start_ip_int AND end_ip_int
    join ags_games.raw.time_of_day_lu lu
    on hour(game_event_ltz)= lu.hour;

-- NOW CHECK THE DATA PRESENT IN THE LOAD_LOGS_TABLE BY EXECUTING THE TASK MANUALLY.
--make a note of how many rows you have in the table
select count(*)
from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

--Run the task to load more rows
execute task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

--check to see how many rows were added (if any!)
select count(*)
from AGS_GAMES.ENHANCED.LOGS_ENHANCED; 

--IDEMPOTENT

-- IF WE WANT TO MAKE SURE THAT THE TABLE IS GETTING UPDATED TIMELY AND ALSO CONTAINING UNIQUE RECORDS 
-- THEN WE NEED TO DO SOMETHING LIKE THIS (DELETING + ADDING RECORDS) AGAIN   


--first we dump all the rows out of the table
truncate table ags_games.enhanced.LOGS_ENHANCED;

--then we put them all back in
INSERT INTO ags_games.enhanced.LOGS_ENHANCED (
SELECT logs.ip_address 
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
, CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
, DAYNAME(game_event_ltz) as DOW_NAME
, TOD_NAME
from ags_games.raw.LOGS logs
JOIN ipinfo_geoloc.demo.location loc 
ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
JOIN ags_games.raw.TIME_OF_DAY_LU tod
ON HOUR(game_event_ltz) = tod.hour);

--clone the table to save this version as a backup
--since it holds the records from the UPDATED FEED file, we'll name it _UF
create table ags_games.enhanced.LOGS_ENHANCED_UF 
clone ags_games.enhanced.LOGS_ENHANCED;

-- FOR THE ABOVE PROBLEM TO SOLVE WE CAN MAKE USE OF SQL MERGE STMT
-- A SQL merge lets you compare new records to already loaded records and do different things based on what you learn by doing the comparison. 

-- LETS DEVELOP THIS MERGE STMT STEP BY STEP
-- THROWS AN ERROR

MERGE INTO ENHANCED.LOGS_ENHANCED e
USING RAW.LOGS r
ON r.user_login = e.GAMER_NAME
WHEN MATCHED THEN
UPDATE SET IP_ADDRESS = 'Hey I updated matching rows!';

-- CORRECTING THE ERROR

MERGE INTO ags_games.ENHANCED.LOGS_ENHANCED e
USING ags_games.RAW.LOGS r
ON r.user_login = e.GAMER_NAME
and r.datetime_iso8601 = e.game_event_utc
and r.user_event = e.game_event_name
WHEN MATCHED THEN
UPDATE SET IP_ADDRESS = 'Hey I updated matching rows!';

-- Building a new merge command to add data by cobbling together bits of code from previous statements.

MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT logs.ip_address 
                    , logs.user_login as GAMER_NAME
                    , logs.user_event as GAME_EVENT_NAME
                    , logs.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.LOGS logs
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME);


-- LETS CHECK THIS MERGE STATEMENT 
-- truncate so we can start the load over again
truncate table AGS_GAMES.ENHANCED.LOGS_ENHANCED;

-- COUNT NO OF RECORDS PRESENT IN LOGS_ENHANCED
SELECT COUNT(*) FROM AGS_GAMES.ENHANCED.LOGS_ENHANCED; -- 0 RECORDS

-- RUN MERGE STMT WE CREATED
MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT logs.ip_address 
                    , logs.user_login as GAMER_NAME
                    , logs.user_event as GAME_EVENT_NAME
                    , logs.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.LOGS logs
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME);

-- COUNT NO OF RECORDS PRESENT IN LOGS_ENHANCED
SELECT COUNT(*) FROM AGS_GAMES.ENHANCED.LOGS_ENHANCED; -- 160 RECORDS

-- SINCE, WE HAVE CHECKED THAT EVERYTHING IS WORKING FINE WITH OUR MERGE CMD
-- WE CAN CREATE A TASK TO RUN THIS CMND AND TO LOAD THE DATA

create or replace task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED
	warehouse=COMPUTE_WH
	schedule='5 minute'
	as
    MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT logs.ip_address 
                    , logs.user_login as GAMER_NAME
                    , logs.user_event as GAME_EVENT_NAME
                    , logs.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.LOGS logs
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME);


--Testing cycle for MERGE. Use these commands to make sure the Merge works as expected

--Write down the number of records in your table 
select * from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

--Run the Merge a few times. No new rows should be added at this time 
EXECUTE TASK AGS_GAMES.RAW.LOAD_LOGS_ENHANCED;

--Check to see if your row count changed 
select * from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

-- NO MATTER, HOW MANY TIMES WE ARE TRYING TO EXECUTE THE SAME TASK
-- WE CAN MAKE SURE THAT WE ARE NOT ENTERING ANY DUPLICATE RECORDS

-- LETS INSERT A FAKE RECORD
--Insert a test record into your Raw Table 
--You can change the user_event field each time to create "new" records 
--editing the ip_address or datetime_iso8601 can complicate things more than they need to 
--editing the user_login will make it harder to remove the fake records after you finish testing 
INSERT INTO ags_games.raw.game_logs 
select PARSE_JSON('{"datetime_iso8601":"2025-01-01 00:00:00.000", "ip_address":"196.197.196.255", "user_event":"fake event", "user_login":"fake user"}');

--After inserting a new row, run the Merge again 
EXECUTE TASK AGS_GAMEs.RAW.LOAD_LOGS_ENHANCED;

--Check to see if any rows were added 
select * from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

--When you are confident your merge is working, you can delete the raw records 
delete from ags_games.raw.game_logs where raw_log like '%fake user%';

--You should also delete the fake rows from the enhanced table
delete from AGS_GAMES.ENHANCED.LOGS_ENHANCED
where gamer_name = 'fake user';

--Row count should be back to what it was in the beginning
select * from AGS_GAMEs.ENHANCED.LOGS_ENHANCED; 

-- LETS TRY TO AUTOMATE THINGS AND OPTIMIZE THE PIPELINE
-- for this, we will perform below things

-- CREATE A STAGE
CREATE STAGE AGS_GAMES.RAW.UNI_KISHORE_PIPELINE 
	URL = 's3://uni-kishore-pipeline' 
	DIRECTORY = ( ENABLE = true );

-- CREATING A PL_GAME_LOGS TABLE
create or replace TABLE AGS_GAMES.RAW.PL_GAME_LOGS (
	RAW_LOG VARIANT
);

-- POPULATING DATA TO PL_GAME_LOGS TABLE
copy into AGS_GAMES.RAW.PL_GAME_LOGS
from @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
file_format = (format_name=AGS_GAMES.RAW.FF_JSON_LOGS);

-- CREATING A TASK
create or replace task AGS_GAMES.RAW.GET_NEW_FILES
	schedule='10 minute'
	WAREHOUSE = 'COMPUTE_WH'
	as 
    copy into AGS_GAMES.RAW.PL_GAME_LOGS
    from @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
    file_format = (format_name=AGS_GAMES.RAW.FF_JSON_LOGS);
    
-- EXECUTING TASK
EXECUTE TASK AGS_GAMES.RAW.GET_NEW_FILES;

--COPY & PASTING LOGS-VIEW DEFINITION
create or replace view AGS_GAMES.RAW.LOGS(
	IP_ADDRESS,
	USER_EVENT,
	USER_LOGIN,
	DATETIME_ISO8601,
	RAW_LOG
) as
select 
    raw_log:ip_address::text as ip_address,
    raw_log:user_event::text as user_event,
    raw_log:user_login::text as user_login,
    raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
    *
from AGS_GAMES.RAW.GAME_LOGS
where RAW_LOG:ip_address::text is not null;

-- BASED ON THIS, WE WILL CREATE ANOTHER VIEW CALLED PL_LOGS
create view AGS_GAMES.RAW.pl_logs as
select 
    raw_log:ip_address::text as ip_address,
    raw_log:user_event::text as user_event,
    raw_log:user_login::text as user_login,
    raw_log:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
    *
from AGS_GAMES.RAW.pl_GAME_LOGS
where RAW_LOG:agent::text is null;

-- QUERYING DATA FROM PL_LOGS VIEW
select * from pl_logs;

-- MODIFYING MERGE TASK
MERGE INTO ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT pl.ip_address 
                    , pl.user_login as GAMER_NAME
                    , pl.user_event as GAME_EVENT_NAME
                    , pl.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,pl.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.pl_LOGS pl
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(pl.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(pl.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME); --list of columns (but we can mark as coming from the r select);

-- viewing data
select * from AGS_GAMES.RAW.LOGS;
select * from AGS_GAMES.RAW.PL_LOGS;

-- CREATE A RESOURCE MONITOR TO KEEP TRACK OF CREDITS WHILE EXECUTING TASKS IN THE BACKGROUND

-- TRUNCATE THE TARGET TABLE
TRUNCATE TABLE AGS_GAMES.ENHANCED.LOGS_ENHANCED;

-- RESUME/START THE TASKS CREATED
--Turning on a task is done with a RESUME command
alter task AGS_GAMES.RAW.GET_NEW_FILES resume;
alter task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED resume;

--Turning OFF a task is done with a SUSPEND command
alter task AGS_GAMES.RAW.GET_NEW_FILES suspend;
alter task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED suspend;

-- VALIDATING OUR TASKS BY CONFIRMING NO OF RECORDS PRESENT
--Step 1 - how many files in the bucket?
list @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE; --116 records

--Step 2 - number of rows in raw table (should be file count x 10)
select count(*) from AGS_GAMES.RAW.PL_GAME_LOGS; --1160 records

--Step 3 - number of rows in raw table (should be file count x 10)
select count(*) from AGS_GAMES.RAW.PL_LOGS; --1160 records

--Step 4 - number of rows in enhanced table (should be file count x 10 but fewer rows is okay because not all IP addresses are available from the IPInfo share)
select count(*) from AGS_GAMES.ENHANCED.LOGS_ENHANCED;

-- LETS IMPROVE THE TASKS BY CREATING DEPENDENCIES BETWEEN THEM INSTEAD OF SCHEDULING
-- BEFORE DOING THIS UPDATE WE NEED TO GRANT ACCESS OF MANAGED TASKS
use role accountadmin;

grant EXECUTE MANAGED TASK on account to SYSADMIN;

--switch back to sysadmin
use role sysadmin;

-- TASK 1:
create or replace task AGS_GAMES.RAW.GET_NEW_FILES
    schedule='5 minute'
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'
	as 
    copy into AGS_GAMES.RAW.PL_GAME_LOGS
    from @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
    file_format = (format_name=AGS_GAMES.RAW.FF_JSON_LOGS);

-- TASK 2:
create or replace task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'
	after AGS_GAMES.RAW.GET_NEW_FILES
	as 
    MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT logs.ip_address 
                    , logs.user_login as GAMER_NAME
                    , logs.user_event as GAME_EVENT_NAME
                    , logs.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.LOGS logs
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME);


-- FEW MORE STEPS TO BE DONE ARE:

SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs');

-- CREATING A TABLE WITH ABOVE SELECT STMNTS
create table AGS_GAMES.RAW.ED_PIPELINE_LOGS as (
SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs'));

--truncate the table rows that were input during the CTAS, if that's what you did
truncate table ED_PIPELINE_LOGS;

--reload the table using your COPY INTO
COPY INTO ED_PIPELINE_LOGS
FROM (
    SELECT 
    METADATA$FILENAME as log_file_name 
  , METADATA$FILE_ROW_NUMBER as log_file_row_id 
  , current_timestamp(0) as load_ltz 
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
)
file_format = (format_name = ff_json_logs);

-- CREATING OUR OWN SNOWPIPE OBJECT TO INGEST NEW DATA TO MAKE OUR PIPELINE EVENT-DRIVEN
CREATE OR REPLACE PIPE PIPE_GET_NEW_FILES
auto_ingest=true
aws_sns_topic='arn:aws:sns:us-west-2:321463406630:dngw_topic'
AS 
COPY INTO ED_PIPELINE_LOGS
FROM (
    SELECT 
    METADATA$FILENAME as log_file_name 
  , METADATA$FILE_ROW_NUMBER as log_file_row_id 
  , current_timestamp(0) as load_ltz 
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAMES.RAW.UNI_KISHORE_PIPELINE
)
file_format = (format_name = ff_json_logs);

-- Update the LOAD_LOGS_ENHANCED Task
-- BEFORE UPDATING, WE CAN CREATE A COPY OF THE EXISTING TABLE
create table ags_games.enhanced.LOGS_ENHANCED_backup
clone ags_games.enhanced.LOGS_ENHANCED;

-- Your old pipeline had two tasks. Your new pipeline uses one task and one Snowpipe. The task that continues to be used will need to be edited for the new configuration. 

--truncating logs enhanced table
truncate table AGS_GAMES.ENHANCED.LOGS_ENHANCED;

--VALIADATE THE DATA
select * from AGS_GAMES.ENHANCED.LOGS_ENHANCED;
select * from AGS_GAMES.ENHANCED.LOGS_ENHANCED_backup;

-- Edit the LOAD_LOGS_ENHANCED Task so it loads from ED_PIPELINE_LOGS instead of PL_LOGS . 
create or replace task AGS_GAMES.RAW.LOAD_LOGS_ENHANCED
	schedule='5 minute'
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE='XSMALL'
	as MERGE INTO ENHANCED.LOGS_ENHANCED e
        USING (
               SELECT logs.ip_address 
                    , logs.user_login as GAMER_NAME
                    , logs.user_event as GAME_EVENT_NAME
                    , logs.datetime_iso8601 as GAME_EVENT_UTC
                    , city
                    , region
                    , country
                    , timezone as GAMER_LTZ_NAME
                    , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
                    , DAYNAME(game_event_ltz) as DOW_NAME
                    , TOD_NAME
                    from ags_games.raw.ED_PIPELINE_LOGS logs
                    JOIN ipinfo_geoloc.demo.location loc 
                    ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
                    AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
                    BETWEEN start_ip_int AND end_ip_int
                    JOIN ags_games.raw.TIME_OF_DAY_LU tod
                    ON HOUR(game_event_ltz) = tod.hour
        ) r --we'll put our fancy select here
        ON r.gamer_name = e.GAMER_NAME
        and r.GAME_EVENT_UTC = e.game_event_utc
        and r.GAME_EVENT_NAME = e.game_event_name
        WHEN NOT MATCHED THEN
        insert (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY,
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME) --list of columns
        values (
                IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME,
                GAME_EVENT_UTC, CITY, REGION, COUNTRY, 
                GAMER_LTZ_NAME, GAME_EVENT_LTZ,
                DOW_NAME, TOD_NAME);

-- CMND TO CHECK PIPE IS RUNNING OR NOT
select parse_json(SYSTEM$PIPE_STATUS( 'ags_games.raw.PIPE_GET_NEW_FILES' ));

-- cmnd to start the pipe
ALTER PIPE ags_game_audience.raw.PIPE_GET_NEW_FILES REFRESH;

-- CREATING STREAMS TO MAKE THE PIPELINE MORE EVENT DRIVEN

--create a stream that will keep track of changes to the table
create or replace stream ags_games.raw.ed_cdc_stream 
on table AGS_GAMES.RAW.ED_PIPELINE_LOGS;

--look at the stream you created
show streams;

--check to see if any changes are pending (expect FALSE the first time you run it)
--after the Snowpipe loads a new file, expect to see TRUE
select system$stream_has_data('ed_cdc_stream');

--query the stream
select * 
from ags_games.raw.ed_cdc_stream;  

--check to see if any changes are pending
select system$stream_has_data('ed_cdc_stream');

--if your stream remains empty for more than 10 minutes, make sure your PIPE is running
select SYSTEM$PIPE_STATUS('PIPE_GET_NEW_FILES');

--process the stream by using the rows in a merge 
MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
USING (
        SELECT cdc.ip_address 
        , cdc.user_login as GAMER_NAME
        , cdc.user_event as GAME_EVENT_NAME
        , cdc.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,cdc.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_games.raw.ed_cdc_stream cdc
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(cdc.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(cdc.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN AGS_GAMES.RAW.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
      ) r
ON r.GAMER_NAME = e.GAMER_NAME
AND r.GAME_EVENT_UTC = e.GAME_EVENT_UTC
AND r.GAME_EVENT_NAME = e.GAME_EVENT_NAME 
WHEN NOT MATCHED THEN 
INSERT (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME)
        VALUES
        (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME);
 
--Did all the rows from the stream disappear? 
select * 
from ags_game_audience.raw.ed_cdc_stream; 


--Create a new task that uses the MERGE you just tested
create or replace task AGS_GAMES.RAW.CDC_LOAD_LOGS_ENHANCED
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE='XSMALL'
	SCHEDULE = '5 minutes'
    when
        system$stream_has_data('ed_cdc_stream')
	as 
    MERGE INTO AGS_GAMES.ENHANCED.LOGS_ENHANCED e
    USING (
        SELECT cdc.ip_address 
        , cdc.user_login as GAMER_NAME
        , cdc.user_event as GAME_EVENT_NAME
        , cdc.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,cdc.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_games.raw.ed_cdc_stream cdc
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(cdc.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(cdc.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN AGS_GAMES.RAW.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
      ) r
ON r.GAMER_NAME = e.GAMER_NAME
AND r.GAME_EVENT_UTC = e.GAME_EVENT_UTC
AND r.GAME_EVENT_NAME = e.GAME_EVENT_NAME 
WHEN NOT MATCHED THEN 
INSERT (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME)
        VALUES
        (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME);
        
--Resume the task so it is running
alter task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED resume;

-- CREATING A CURATED SCHEMA FOR DASHBOARDS AND OTHER STUFFS

CREATE SCHEMA CURATED;

-- SOME SQL QUERIES THAT CAN BE USED TO CREATE DASHBOARDS
select tod_name as time_of_day
           , count(*) as tally
     from ags_game_audience.enhanced.logs_enhanced_uf 
     group by  tod_name
     order by tally desc;

--the ListAgg function can put both login and logout into a single column in a single row
-- if we don't have a logout, just one timestamp will appear
select GAMER_NAME
      , listagg(GAME_EVENT_LTZ,' / ') as login_and_logout
from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED 
group by gamer_name;


select GAMER_NAME
       ,game_event_ltz as login 
       ,lead(game_event_ltz) 
                OVER (
                    partition by GAMER_NAME 
                    order by GAME_EVENT_LTZ
                ) as logout
       ,coalesce(datediff('mi', login, logout),0) as game_session_length
from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED
order by game_session_length desc;


--We added a case statement to bucket the session lengths
select case when game_session_length < 10 then '< 10 mins'
            when game_session_length < 20 then '10 to 19 mins'
            when game_session_length < 30 then '20 to 29 mins'
            when game_session_length < 40 then '30 to 39 mins'
            else '> 40 mins' 
            end as session_length
            ,tod_name
from (
select GAMER_NAME
       , tod_name
       ,game_event_ltz as login 
       ,lead(game_event_ltz) 
                OVER (
                    partition by GAMER_NAME 
                    order by GAME_EVENT_LTZ
                ) as logout
       ,coalesce(datediff('mi', login, logout),0) as game_session_length
from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED_UF)
where logout is not null;